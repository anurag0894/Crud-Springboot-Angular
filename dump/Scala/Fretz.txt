Hi Anurag

Attahced are code snippet which you can use, but make sure you execute these code snippet in shell ,see it works and add necessary error handling , Going forward pls adhere to coding standards, Please take help from karthik  in case you are not aware.
Going forward codes which are not written by coding standards will not be accepted

Function to create Schema

def colType(datatype:String) = {
   datatype.toLowerCase match  { 
  case "string" => org.apache.spark.sql.types.StringType
  case "integer" => org.apache.spark.sql.types.IntegerType
  case "date" => org.apache.spark.sql.types.DateType
  case "short" => org.apache.spark.sql.types.ShortType
  case "long" => org.apache.spark.sql.types.LongType
  case "float" => org.apache.spark.sql.types.FloatType
  case "timestamp" => org.apache.spark.sql.types.TimestampType
  case "boolean" => org.apache.spark.sql.types.BooleanType 
  case _ => org.apache.spark.sql.types.StringType
} 
 }

val schema_create = (arryField:Array[(String,String)]) => {
StructType(arr1.map(row => StructField(row._1,colType(row._2),true)))}

val schemaArray = Array(("name","String"),("age","Integer") ,("sex","Boolean"), ("DOB","date")     )
val schema = schema_create(schemaArray)

Function to replicate records in DF

def generateSeq(counter: Int) =  {
  if(counter == 0) Array(1) else  (1 to counter).toArray
}
val udf_generateSeq = udf(generateSeq _)

val t1 = df.withColumn("exploded_col", explode(udf_generateSeq(col("count"))))
                       .withColumn("col1",$"col1" * $"exploded_col").drop("exploded_col")


Function to generate DFs and to do union

val sqlArray:Array[String] = for (tbl<-srctableName) yield {
  val sql= s""" select coalesce(concat(CNTRY_CD,CNTRY_SHORT_NAME),'?'),* from $db_name.$tbl """
  sql
}
//Generate DF in an array based on above SQL
val dfArray:Array[DataFrame] = for (sql<-sqlArray)yield {
        val df = spark.sql(sql)
        df
}
val dfFinal :DataFrame = if (dfArray.size == 1) dfArray(0) else dfArray.reduce(_ union _ )


Function to transpose set of columns and to generate additional column

//val df = sc.parallelize(Seq(("fretz","ge",1,2,0,2,1),("vivek","google",12,23,4,5,6),("maninder","Facebook",123,23,7,8,9))).toDF("name","company","month1","month2","month3","month4","month10")

val udf_zip = udf((col1:Seq[String] ,col2:Seq[Int]) => col1.zip(col2))
val colNames = Seq( "month1","month2","month3","month4","month10")
val colId = Seq( 1,2,3,4,10)
val df_exploded = df.withColumn("temp", explode ( udf_zip(array((colNames.map(lit)):_* ) , array((colId.map(lit)):_* ) ) ) )
          .withColumn("month", $"temp._1" )
          .withColumn("key_code",$"temp._2")
          .drop("temp" +:colNames:_*)

//df_exploded.show



From: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Date: Monday, 30 October 2017 at 20:26
To: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>, "Gupta, Pankaj (GE Digital)" <Pankaj4.Gupta@ge.com>
Subject: Re: Code Review

Function for creating custom schema. Add this function in the code to avoid explicitly defining StructField for many times. I will send across on how to explode on 2 columns and replication of dataframes 
 
def colType(datatype:String) = {
   datatype.toLowerCase match  { 
  case "string" => org.apache.spark.sql.types.StringType
  case "integer" => org.apache.spark.sql.types.IntegerType
  case "date" => org.apache.spark.sql.types.DateType
  case "short" => org.apache.spark.sql.types.ShortType
  case "long" => org.apache.spark.sql.types.LongType
  case "float" => org.apache.spark.sql.types.FloatType
  case "timestamp" => org.apache.spark.sql.types.TimestampType
  case "boolean" => org.apache.spark.sql.types.BooleanType 
  case _ => org.apache.spark.sql.types.StringType
} 
 }
 
val schema_create = (arryField:Array[(String,String)]) => {
StructType(arr1.map(row => StructField(row._1,colType(row._2),true)))}
 
/////////////////////
How to call in the code
val schemaArray = Array(("name","String"),("age","Integer") ,("sex","Boolean"), ("DOB","date")     )
val schema = schema_create(schemaArray)
 
From: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Date: Friday, 27 October 2017 at 02:37
To: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: RE: Code Review
 
Hi Fretz
 
•	I tried using the csv date format  for date type (Fri Oct 13 00:00:00 EDT 2017) in csv header but it returns null.
•	For Explode to work, I did concatenation of two fields which were changing .
 
Regards
Anurag Choudhary
 
 
 
 
 
From: Nuson, Fretz (GE Digital) [mailto:Fretz.Nuson@ge.com] 
Sent: Thursday, October 26, 2017 2:46 PM
To: Choudhary, Anurag (GE Digital, consultant)
Cc: Debasmita, Mishra (GE Digital, consultant)
Subject: Re: Code Review
Importance: High
 
Hi 
 
Let me know once it is done, I want to do a code review so that necessary adjustments can be done at earliest  
 
From: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Date: Tuesday, 24 October 2017 at 15:03
To: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: Re: Code Review
 
Questions:
1)Does all the CSV file has same schema structure , talking the file that is assumped from CSVFolder via loop
2) Why are we taking .xls and .xml file in very beginning if that is interested with indirect_backlog file
3) How do you read XLS and XML ( code in loop) if they get picked up
4) Final sql to BACKLOG_STAGE needs to be modified to handle Date conversion alone (“ will be handled in csv reader) . Do check “timestampFormat” and “quote” option of CSV reader before you write extra sql
 
And going forward, I will not approve SQL coding for handling non transformation logic -) and use this as a reference or let me know in advance
 
Read http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html to see what other parameter can be set for reading a csv
 
From: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Date: Tuesday, 24 October 2017 at 14:10
To: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: Re: Code Review
 
Work on the explode piece and I rewriting your while loop and then you can use the code which I will send u in a while
 
From: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Date: Tuesday, 24 October 2017 at 14:03
To: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: Re: Code Review
 
Still we cant use this code , as I mentioned cant use SQL for doing logic which are not transformation or aggregation. Below is the modified code and I still trying figure out the use of tables. I
 
  import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};
 
 
  def readCSV(path:String,fileSchema:StructType,isHeader:Boolean) :org.apache.spark.sql.DataFrame = {
    val df =  spark.read.format("csv")
                .option("ignoreLeadingWhiteSpace" ,true)
                .option("ignoreTrailingWhiteSpace" ,true)
                .option("inferSchema" ,false)
                .option("header" ,isHeader)
                .option("timestampFormat" ,"yyyy-MM-dd HH:mm:ss")
                .schema(fileSchema)
                .load(path)
        df
  }
 
  /*spark.sql("LOAD DATA LOCAL INPATH '/home/rajeebp/Manual_Adjustments/SrcFiles/XLSFiles/filelist.csv' OVERWRITE INTO TABLE g00103.Manual_Adjustments")
  spark.sql("""insert overwrite table g00103.SFTP_NEWFILE_TEST
  select * from g00103.Manual_Adjustments
  where SUBSTR(FILENAME,-4,4)='.xls' or SUBSTR(FILENAME,-4,4)='.xml'""")*/
 
  //Code to read the CSV filelist.csv from HDFS and filtering only XLS and XML contents from the file
    val filelist_schema = StructType(Array(StructField("filename",StringType,true)))
    val filelist_path = "/tmp/manual_adjustments/XLSFiles/filelist.csv"
    val filelist_ds = readCSV(filelist_path,filelist_schema,false).as[String]
    val filelist_ds_filtered = filelist_ds.filter( col("filename").endsWith("xls") || col("filename").endsWith("xml"))  //SFTP_NEWFILE_TEST
 
/*
spark.sql("LOAD DATA LOCAL INPATH '/home/rajeebp/Manual_Adjustments/SrcFiles/Indirect_BACKLOG.txt' OVERWRITE INTO TABLE g00103.Indirect_BACKLOG_temp");
*/
 
  //Code to read the CSV indirect_backlog.txt from HDFS
    val indirectBacklog_schema = StructType(Array(StructField("filename",StringType,true)))
    val indirectBacklog_path = "/tmp/manual_adjustments/Indirect_BACKLOG.txt"
    val indirectBacklog_ds = readCSV(indirectBacklog_path,indirectBacklog_schema,false).as[String]  //Indirect_BACKLOG_temp
 
//Dataset for existing table SFTP_EXISTING_LIST
val sftp_existing_list_ds = spark.sql("select filename from g00103.sftp_existing_list").as[String].cache  //SFTP_EXISTING_LIST
 
/*spark.sql("""insert overwrite table g00103.Indirect_BACKLOG
            select FILENAME from g00103.Indirect_BACKLOG_temp
               intersect
             (select FILENAME from g00103.SFTP_NEWFILE_TEST
               except
             select FILENAME from g00103.SFTP_EXISTING_LIST)""")*/
 
val newfile_ds = filelist_ds_filtered.except(sftp_existing_list_ds)  //List of new files
newfile_ds.createOrReplaceTempView("newfile_ds") //registering it as temp table
//Finding the common file new file list & indirect backlog
val newfile_ds_common = indirectBacklog_ds.intersect(newfile_ds) // Indirect_BACKLOG
 
 
/*spark.sql("""insert into table g00103.SFTP_EXISTING_LIST
             select FILENAME from g00103.SFTP_NEWFILE_TEST
               except
             select FILENAME from g00103.SFTP_EXISTING_LIST""")*/
             
//Append new file list to existing table sftp_existing_list only if there is new files to be added
if newfile_ds.count >0
  spark.sql("insert into table g00103.sftp_existing_list select filename from newfile_ds")
 
/*  
spark.sql("insert overwrite table g00103.tmp_BACKLOG  select * from g00103.Indirect_BACKLOG")
*/
 
 
/*
var a=spark.sql("select * from g00103.Indirect_BACKLOG").count;
*/
//Find the number of files that needs to be appended to a tableName
 
From: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Date: Tuesday, 24 October 2017 at 14:01
To: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: RE: Code Review
 
Hi Fretz
 
I have modified the code inside loop. Eliminating cross join, except and temp tables in loop.
Previously the code was based on selecting one filename from Indirect_backlog using MAX and in the latter phase have an EXCEPT to the data so the same file won’t get processed again but there was a lot I/O going on.
 
Now I have counter on the filename.
 
Ex..  11708623_273692.csv   1
         11321837_138768.csv   2
 
Filter inside loop instead of MAX.
There are 3 temp tables in the code since the functions are restricted with LOAD in overwrite mode.
 
Regards
Anurag Choudhary
From: Nuson, Fretz (GE Digital) [mailto:Fretz.Nuson@ge.com] 
Sent: Tuesday, October 24, 2017 12:31 PM
To: Choudhary, Anurag (GE Digital, consultant)
Cc: Debasmita, Mishra (GE Digital, consultant)
Subject: Re: Code Review
 
This code needs to be rewritten using scala +dataframe . are 28 references to hive table and it needs to be rewritten . I will send you the modified code snippet
 
And use SQL only for transformation & aggregation and should  scala code for the logic manipulation. If there are more such codes, pls bring it to my attention as we cant move them to prd
 
From: "Choudhary, Anurag (GE Digital, consultant)" <anurag.choudhary@ge.com>
Date: Monday, 23 October 2017 at 21:50
To: "Nuson, Fretz (GE Digital)" <Fretz.Nuson@ge.com>
Cc: "Debasmita, Mishra (GE Digital, consultant)" <mishra.debasmita@ge.com>
Subject: Code Review
 
Hi Fretz
 
Please find the code of manual adjustment for .csv to stage table load.
 
Regards
Anurag Choudhary
 
