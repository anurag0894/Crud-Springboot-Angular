rc4-hmac---protocol works well with talend for kerberos

__________________________________________________________________________________

Scala.collection.mutable package contains all the mutable collections. You can add, remove and update data while using this package.

scala> val map = scala.collection.mutable.HashMap.empty[Int,String]
map: scala.collection.mutable.HashMap[Int,String] = Map()
scala> map += (1 -> "make a web site")
res42: map.type = Map(1 -> make a web site)
scala> map += (3 -> "profit!")
res43: map.type = Map(1 -> make a web site, 3 -> profit!)
scala> map(1)
res44: String = make a web site
scala> map contains 2
res46: Boolean = false
__________________________________________________________________________________

OOPS aproach//for saving it as .scala extension

object ScalaExample{  
    def main(args:Array[String]){  
        println "Hello Scala"  
    }  
}  

This file is saved with the name ScalaExample.scala.

Command to compile this code is: scalac ScalaExample.scala

Command to execute the compiled code is: scala ScalaExample
__________________________________________________________________________________

functional approach

def scalaExample{  
    println("Hello Scala")  
}


__________________________________________________________________________________

val immutable,
var muttable

__________________________________________________________________________________

var number:Int = 85 

 if(number % 2==0)
     | {
     | println("hi")
     | }else    //don't hit enter after },otherwise it will come out of if have } else at the same line
     | {
     | prinln("bye")
     | }

__________________________________________________________________________________


var number:Int = 85  
if(number>=0 && number<50){  
    println ("fail")  
}  else if(number>=50 && number<60){  
    println("D Grade")  
}  else if(number>=60 && number<70){  
    println("C Grade")  
}  else if(number>=70 && number<80){  
    println("B Grade")  
}  else if(number>=80 && number<90){  
    println("A Grade")  
}  else if(number>=90 && number<=100){  
    println("A+ Grade")  
}  else println ("Invalid")  
	 
__________________________________________________________________________________

object MainObject {  
   def main(args: Array[String]) {  
      val result = checkIt(-10)  
      println (result)  
   }  
    def checkIt (a:Int)  =  if (a >= 0) 1 else -1    // Passing a if expression value to function  
}  

-1

__________________________________________________________________________________

pattern matching

object MainObject {  
   def main(args: Array[String]) {  
        var result = search ("Hello")  
        print(result)  
    }  
    def search (a:Any):Any = a match{  
        case 1  => println("One")  
        case "Two" => println("Two")  
        case "Hello" => println("Hello")  
        case _ => println("No")  
              
        }  
}  

__________________________________________________________________________________

for loop (to keyword) 

object MainObject {  
   def main(args: Array[String]) {  
        for( a <- 1 to 10 ){  
         println(a);  
      }  
   }  
}

1
2
3
4
5
6
7
8
9
10


for loop (until keyword)

object MainObject {  
   def main(args: Array[String]) {  
        for( a <- 1 until 10 ){  
         println(a);  
      }  
   }  
}  


1
2
3
4
5
6
7
8
9


def main(args: Array[String]) {  
        for( a <- 1 to 10 if a%2==0 ){  
         println(a);  
      }  
   }  
}  


2
4
6
8
10
__________________________________________________________________________________


scala> var list=List(1 to 10)
list: List[scala.collection.immutable.Range.Inclusive] = List(Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))  (it is a range)


scala> for(a<-list)
     | {
     | println(a)}
Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

scala>  var list = List(1,2,3,4,5,6,7,8,9)
list: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)   (it is list of int)

scala> for(a<-list)
     |      | {
     |      | println(a)}
1
2
3
4
5
6
7
8
9

__________________________________________________________________________________


object MainObject {  
   def main(args: Array[String]) {  
        var list = List(1,2,3,4,5,6,7,8,9)  // Creating a list  
        list.foreach{  
            println     // Print each element  
        }  
        list.foreach(print)  
        println  
        list.foreach((element:Int)=>print(element+" "))      // Explicitly mentioning type of elements  
   }  
}  



1
2
3
4
5
6
7
8
9
123456789
1 2 3 4 5 6 7 8 9


__________________________________________________________________________________

by for skipping iteration

object MainObject {  
   def main(args: Array[String]) {  
        for(i<-1 to 10 by 2){  
            println(i)  
        }  
   }  
}  

1
3
5
7
9

__________________________________________________________________________________

Scala comments

//--single line

/*

*/--multiline comments


__________________________________________________________________________________

Scala Functions

Declaration

def functionName(parameters : typeofparameters) : returntypeoffunction = {  
// statements to be executed  
}  

In the above syntax, = (equal) operator is looking strange but don't worry scala has defined it as:

You can create function with or without = (equal) operator. If you use it, function will return value. If you don't use it, your function will not return anything and will work like subroutine.

Scala functions don?t use return statement. Return type infers by compiler from the last expression or statement present in the function.

1.without =

object MainObject {  
   def main(args: Array[String]) {  
        functionExample()           // Calling function  
    }  
    def functionExample()  {        // Defining a function  
          println("This is a simple function")  
    }  
}  

This is a simple function


2.with =

object MainObject {  
   def main(args: Array[String]) {  
        var result = functionExample()          // Calling function  
        println(result)  
    }  
    def functionExample() = {       // Defining a function  
          var a = 10  
          a  
    }  
}  

10


3.Parameterized Functions

object MainObject {  
   def main(args: Array[String]) = {  
        functionExample(10,20)   
    }  
    def functionExample(a:Int, b:Int) = {  
          var c = a+b  
          println(c)  
    }  
}  

30

4.Recursive function

object MainObject {  
   def main(args: Array[String]) = {  
        var result = functionExample(15,2)   
        println(result)  
    }  
    def functionExample(a:Int, b:Int):Int = {  
        if(b == 0)          // Base condition  
         0  
        else  
         a+functionExample(a,b-1)  
    }  
}  

30

5.Function Named parameters

object MainObject {  
   def main(args: Array[String]) = {  
        var result1 = functionExample(a = 15, b = 2)    // Parameters names are passed during call  
        var result2 = functionExample(b = 15, a = 2)    // Parameters order have changed during call  
        var result3 = functionExample(15)             // Only values are passed during call  
        println(result1+"\n"+result2+"\n"+result3)  
    }  
    def functionExample(a:Int=0, b:Int=0):Int = {  
        a+b  
    }  
}  

17
17
15


6.Higher order functions

object MainObject {  
   def main(args: Array[String]) = {  
     functionExample(25, multiplyBy2)                   // Passing a function as parameter  
    }  
    def functionExample(a:Int, f:Int=>AnyVal):Unit = {  
        println(f(a))                                   // Calling that function   
    }  
    def multiplyBy2(a:Int):Int = {  
        a*2  
    }  
}  

vote 91. =>(rocket) is syntactic sugar for creating instances of functions. Recall that every function in scala is an instance of a class.
 For example, the type Int => String , is equivalent to the type Function1[Int,String]
 i.e. a function that takes an argument of type Int and returns a String 
 
7.Anonymous Function
 
 object MainObject {  
   def main(args: Array[String]) = {  
     var result1 = (a:Int, b:Int) => a+b        // Anonymous function by using => (rocket)  
     var result2 = (_:Int)+(_:Int)              // Anonymous function by using _ (underscore) wild card  
     println(result1(10,10))  
     println(result2(10,10))  
    }  
}  

8.Function with Variable Length Parameters

def add(args: Int*) = {  
    var sum = 0;  
    for(a <- args) sum+=a  
    sum  
}  
var sum = add(1,2,3,4,5,6,7,8,9);  
println(sum);  ---45

__________________________________________________________________________________

1.
class Student(id:Int, name:String){  
    def getRecord(){  
        println(id+" "+name);  
    }  
}  
  
object MainObject{  
    def main(args: Array[String]){  
        var student1 = new Student(101,"Raju");  
        var student2 = new Student(102,"Martin");  
        student1.getRecord();  
        student2.getRecord();  
    }  
}  

2.

Scala Companion Object

In scala, when you have a class with same name as singleton object, it is called companion class and the singleton object is called companion object.

The companion class and its companion object both must be defined in the same source file.


class ComapanionClass{  
    def hello(){  
        println("Hello, this is Companion Class.")  
    }  
}  
object CompanoinObject{  
    def main(args:Array[String]){  
        new ComapanionClass().hello()  
        println("And this is Companion Object.")  
    }  
}  

__________________________________________________________________________________
1.

Scala Case Class Example


case class CaseClass(a:Int, b:Int) 


object MainObject{  
    def main(args:Array[String]){  
        var c =  CaseClass(10,10)       // Creating object of case class  
        println("a = "+c.a)               // Accessing elements of case class  
        println("b = "+c.b)  
    }  
}

2.

trait SuperTrait  
case class CaseClass1(a:Int,b:Int) extends SuperTrait  
case class CaseClass2(a:Int) extends SuperTrait         // Case class  
case object CaseObject extends SuperTrait               // Case object  
object MainObject{  
    def main(args:Array[String]){  
        callCase(CaseClass1(10,10))  
        callCase(CaseClass2(10))  
        callCase(CaseObject)  
    }  
    def callCase(f:SuperTrait) = f match{  
        case CaseClass1(f,g)=>println("a = "+f+" b ="+g)  
        case CaseClass2(f)=>println("a = "+f)  
        case CaseObject=>println("No Argument")  
    }  
}  

__________________________________________________________________________________

1. Scala Primary Constructor

Scala provides a concept of primary constructor with the definition of class. You don't need to define explicitly constructor if 
your code has only one constructor. It helps to optimize code. You can create primary constructor with zero or more parameters.

class Student(id:Int, name:String){  
    def showDetails(){  
        println(id+" "+name);  
    }  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        var s = new Student(101,"Rama");  
        s.showDetails()  
    }  
}  


2. Scala Secondary (auxiliary) Constructor

//when you create object...this pointer gets fire...then this passes values to the constructor

You can create any number of auxiliary constructors in a class. You must call primary constructor from inside the auxiliary constructor. 
this keyword is used to call constructor from other constructor. When calling other constructor make it first line in your constructor.

class Student(id:Int, name:String){  
    var age:Int = 0  
    def showDetails(){  
        println(id+" "+name+" "+age)  
    }  
    def this(id:Int, name:String,age:Int){  
        this(id,name)       // Calling primary constructor, and it is first line  
        this.age = age  
    }  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        var s = new Student(101,"Rama",20);  //you mayn't pass 20...default is 0
        s.showDetails()  
    }  
}  

3. Scala Example: Constructor Overloading


class Student(id:Int){  
    def this(id:Int, name:String)={  
        this(id)  
        println(id+" "+name)  
    }  
    println(id)  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        new Student(101)  
        new Student(100,"India")  
    }  
}  


__________________________________________________________________________________

function override

class Bank{    
        def getRateOfInterest()={  
            0  
        }    
    }    
        
    class SBI extends Bank{    
        override def getRateOfInterest()={  
         8  
        }    
    }    
        
    class ICICI extends Bank{    
        override def getRateOfInterest()={  
            7  
        }    
    }    
      
    class AXIS extends Bank{    
        override def getRateOfInterest()={  
            9  
        }    
    }    
        
    object MainObject{    
        def main(args:Array[String]){    
            var s=new SBI();    
            var i=new ICICI();    
            var a=new AXIS();    
            println("SBI Rate of Interest: "+s.getRateOfInterest());    
            println("ICICI Rate of Interest: "+i.getRateOfInterest());    
            println("AXIS Rate of Interest: "+a.getRateOfInterest());    
        }    
    }    
Output:

SBI Rate of Interest: 8
ICICI Rate of Interest: 7
AXIS Rate of Interest: 9


__________________________________________________________________________________

1.Single D array

class ArrayExample{  
    var arr = new Array[Int](5)         // Creating single dimensional array  
    def show(){  
        for(a<-arr){                      // Traversing array elements  
            println(a)  
        }  
        println("Third Element before assignment = "+ arr(2))        // Accessing elements by using index  
        arr(2) = 10                                                          // Assigning new element at 2 index  
        println("Third Element after assignment = "+ arr(2))  
    }  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        var a = new ArrayExample()  
        a.show()  
    }  
}  
Output:

0
0
0
0
0
Third Element before assignment = 0
Third Element after assignment = 10

  var arr = Array(1,2,3,4,5) 
  
  
2.Scala Passing Array into Function


class ArrayExample{  
    def show(arr:Array[Int]){  
        for(a<-arr)                // Traversing array elements  
            println(a)  
        println("Third Element = "+ arr(2))        // Accessing elements by using index  
    }  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        var arr = Array(1,2,3,4,5,6)    // creating single dimensional array  
        var a = new ArrayExample()  
        a.show(arr)                     // passing array as an argument in the function  
    }  
}  

1
2
3
4
5
6
Third Element = 3

3.foreach

class ArrayExample{  
    var arr = Array(1,2,3,4,5)      // Creating single dimensional array  
    arr.foreach((element:Int)=>println(element))       // Iterating by using foreach loop  
}  
  
object MainObject{  
    def main(args:Array[String]){  
        new ArrayExample()  
    }  
}  

  
SANDBOX
__________________________________________________________________________________



spark.sql("use g00103")



df.select($"education",$"yearly_income").withColumn("yearly_income1",substring($"yearly_income",9,2)).groupBy($"education").agg("yearly_income1"->"avg").drop($"education")



__________________________________________________________________________________

Databricks

// Create the case classes for our domain
case class Department(id: String, name: String);
case class Employee(firstName: String, lastName: String, email: String, salary: Int);
case class DepartmentWithEmployees(department: Department, employees: Seq[Employee]);

// Create the Departments
val department1 = new Department("123456", "Computer Science");
val department2 = new Department("789012", "Mechanical Engineering");
val department3 = new Department("345678", "Theater and Drama");
val department4 = new Department("901234", "Indoor Recreation");

// Create the Employees
val employee1 = new Employee("michael", "armbrust", "no-reply@berkeley.edu", 100000);
val employee2 = new Employee("xiangrui", "meng", "no-reply@stanford.edu", 120000);
val employee3 = new Employee("matei", null, "no-reply@waterloo.edu", 140000);
val employee4 = new Employee(null, "wendell", "no-reply@princeton.edu", 160000);

// Create the DepartmentWithEmployees instances from Departments and Employees
val departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2));
val departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4));
val departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee1, employee4));
val departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3));

Seq is list 

 val departmentWithEmployees1 = new DepartmentWithEmployees(department1,Seq(employee1,employee2))                                              
departmentWithEmployees1: DepartmentWithEmployees = DepartmentWithEmployees(Department(123456,Computer Science),List(Employee(michael,armbrust,no-rep
ly@berkeley.edu,100000), Employee(xiangrui,meng,no-reply@stanford.edu,120000)))

val departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2);
val df1 = departmentsWithEmployeesSeq1.toDF();
display(df1)

val departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4);
val df2 = departmentsWithEmployeesSeq2.toDF();
display(df2)

val unionDF = df1.unionAll(df2)
display(unionDF)

// Remove the file if it exists
dbutils.fs.rm("/tmp/databricks-df-example.parquet", true)
unionDF.write.parquet("/tmp/databricks-df-example.parquet")

val parquetDF = spark.read.parquet("/tmp/databricks-df-example.parquet")

val explodeDF = parquetDF.explode($"employees") {
  case Row(employee: Seq[Row]) => employee.map{ employee =>
    val firstName = employee(0).asInstanceOf[String]
    val lastName = employee(1).asInstanceOf[String]
    val email = employee(2).asInstanceOf[String]
    val salary = employee(3).asInstanceOf[Int]
    Employee(firstName, lastName, email, salary)
  }
}.cache()
display(explodeDF)


import org.apache.spark.sql.Row

val whereDF = explodeDF.where(($"firstName" === "xiangrui") || ($"firstName" === "michael")).sort($"lastName".asc)
display(whereDF)

val naFunctions = explodeDF.na
val nonNullDF = naFunctions.fill("--")
display(nonNullDF)

import org.apache.spark.sql.functions._

// Find the distinct (firstName, lastName) combinations
val countDistinctDF = nonNullDF.select($"firstName", $"lastName")
  .groupBy($"firstName", $"lastName")
  .agg(countDistinct($"firstName") as "distinct_first_names")
  
// Sum up all the salaries
val salarySumDF = nonNullDF.agg("salary" -> "sum")
display(salarySumDF)



:pa

paste mode

CTRL+D


val veryNestedDF = Seq(("1", (2, (3, 4)))).toDF()
veryNestedDF: org.apache.spark.sql.DataFrame = [_1: string, _2: struct<_1: int, _2: struct<_1: int, _2: int>>]


scala> val lol = List(List(1,2), List(3,4))
lol: List[List[Int]] = List(List(1, 2), List(3, 4))

scala> val a = Array(Array(1,2), Array(3,4))
a: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4))

scala> a.flatten
res0: Array[Int] = Array(1, 2, 3, 4)

scala> val couples = List(List("kim", "al"), List("julia", "terry","Kim"))


scala> couples.flatten.map(_.capitalize).sorted.distinct
res8: List[String] = List(Al, Julia, Kim, Terry)


Import the libraries in the first cell

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql._

import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.io.Text //mapper <Text,Text> map(_._2.toString)
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat //for newAPIHadoopFile 


import org.apache.hadoop.conf.Configuration


// Build an example DataFrame dataset to work with.
dbutils.fs.rm("/tmp/dataframe_sample.csv", true)
dbutils.fs.put("/tmp/dataframe_sample.csv", """
id|end_date|start_date|location
1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF
2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD
3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY
4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY
5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-LA
""", true)

val conf = new Configuration
conf.set("textinputformat.record.delimiter", "\n")
val rdd = sc.newAPIHadoopFile("/tmp/dataframe_sample.csv", classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf).map(_._2.toString).filter(_.nonEmpty)

val header = rdd.first()
// Parse the header line
val rdd_noheader = rdd.filter(x => !x.contains("id"))
// Convert the RDD[String] to an RDD[Rows]. Create an array using the delimiter and use Row.fromSeq()
val row_rdd = rdd_noheader.map(x => x.split('|')).map(x => Row.fromSeq(x))   //fromSeq----x is a array of string(SEQUENCE)

val df_schema =
  StructType(
    header.split('|').map(fieldName => StructField(fieldName, StringType, true)))

var df = spark.createDataFrame(row_rdd, df_schema)
df.printSchema

_______________________________________________________________________________________________P
Creating   and writing with filesystem and conf
  
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.io.PrintWriter;

object App {

def main(args : Array[String]) {
println( "Trying to write to HDFS..." )
val conf = new Configuration()
val fs= FileSystem.get(conf)
val output = fs.create(new Path("hdfs://sandbox-hdp.hortonworks.com:8020/tmp/dataframe_sample.csv"))
val writer = new PrintWriter(output)
try {
    writer.write("""
id|end_date|start_date|location
1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF
2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD
3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY
4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY
5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-LA
""") 
    writer.write("\n")
}
finally {
    writer.close()
}
print("Done!")
}

}


//Reading

import scala.io.Source

val fileContents = Source.fromFile("hdfs://sandbox-hdp.hortonworks.com:8020/tmp/dataframe_sample.csv").getLines.mkString



// Instead of registering a UDF, call the builtin functions to perform operations on the columns.
// This will provide a performance improvement as the builtins compile and run in the platform's JVM.

// Convert to a Date type
val timestamp2datetype: (Column) => Column = (x) => { to_date(x) }
val timestamp2datetype=(x:Column) => to_date(x)
df = df.withColumn("date", timestamp2datetype(col("end_date")))

// Parse out the date only
val timestamp2date: (Column) => Column = (x) => { regexp_replace(x," (\\d+)[:](\\d+)[:](\\d+).*$", "") }
df = df.withColumn("date_only", timestamp2date(col("end_date")))

// Split a string and index a field
val parse_city: (Column) => Column = (x) => { split(x, "-")(1) }
val parse_city=(x:Column) => split(x,"-")(1)
df = df.withColumn("city", parse_city(col("location")))

+---+-------------------+-------------------+--------+----------+----+
| id|           end_date|         start_date|location|      date|city|
+---+-------------------+-------------------+--------+----------+----+
|  1|2015-10-14 00:00:00|2015-09-14 00:00:00|   CA-SF|2015-09-14|  SF|
|  2|2015-10-15 01:00:20|2015-08-14 00:00:00|   CA-SD|2015-08-14|  SD|
|  3|2015-10-16 02:30:00|2015-01-14 00:00:00|   NY-NY|2015-01-14|  NY|
|  4|2015-10-17 03:00:20|2015-02-14 00:00:00|   NY-NY|2015-02-14|  NY|
|  5|2015-10-18 04:30:00|2014-04-14 00:00:00|   CA-LA|2014-04-14|  LA|
+---+-------------------+-------------------+--------+----------+----+


// Perform a date diff function
val dateDiff: (Column, Column) => Column = (x, y) => { datediff(to_date(y), to_date(x)) }

df = df.withColumn("date_diff", dateDiff(col("start_date"), col("end_date")))


//I want to convert the DataFrame back to json strings to send back to Kafka.

val rdd_json = df.toJSON
rdd_json.take(2).foreach(println)

My UDF takes a parameter including the column to operate on. How do I pass this parameter?
There is a function available called lit() that creates a static column.

val add_n = udf((x: Integer, y: Integer) => x + y)

// We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.
df = df.withColumn("id_offset", add_n(lit(1000), col("id").cast("int")))
display(df)


val last_n_days = udf((x: Integer, y: Integer) => {
  if (x < y) true else false
})

//last_n_days = udf(lambda x, y: True if x < y else False, BooleanType())

val df_filtered = df.filter(last_n_days(col("date_diff"), lit(90)))
display(df_filtered)

df.select( df("id") > lit(21) )   //lit uses this as column 

 res33.show
+---------+
|(id > 21)|
+---------+
|    false|
|    false|
|    false|
|    false|
|    false|
+---------+


 val last_n_days= (x: Integer, y: Integer)  => {
      if (x < y) true else false
     }

val df_filtered = df.filter(last_n_days(col("date_diff"), lit(90)))

//<console>:61: error: type mismatch;
 found   : org.apache.spark.sql.Column
 required: Integer
       val df_filtered = df.filter(last_n_days(col("date_diff"), lit(90)))

//Can't do this since Integer is in the definition,U have to convert into udf or def function to do this
//You can do 	   
	 
 val last_n_days= (x: Column, y: Column)  => {
      if (x < y) true else false
     }

val df_filtered = df.filter(last_n_days(col("date_diff"), lit(90)))

<console>:45: error: type mismatch;
 found   : org.apache.spark.sql.Column
 required: Boolean
             if (x < y) true else false

//Call is correct,but x<y can't work in column data_type
//But it will work with date functions since date function work of COLUMNS type like earlier

val dateDiff: (Column, Column) => Column = (x, y) => { datediff(to_date(y), to_date(x)) }

//UDF's



scala> def ri(x: Integer, y: Integer):Integer = x+y
ri: (x: Integer, y: Integer)Integer

//More than passing values/names, => is used to define a function literal, which is an alternate syntax used to define a function.

val shortNamedPeople = people.filter( name => name.length < 10 )

//Function Cascading//

def myFunction(f: String => Int): Int = {
  val myString = "Hello!"
  f(myString)
}
// And let's use it. First way:
def anotherFunction(a: String): Int = {
  a.length
}
myFunction(anotherFunction)
// Second way:
myFunction((a: String) => a.length)


val add_n = udf((x: Integer, y: Integer) => x + y)

// We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.
df = df.withColumn("id_offset", add_n(lit(1000), col("id").cast("int")))
display(df)

df.write.partitionBy("end_year", "end_month").parquet("/tmp/sample_table")


__________________________________________________________________________________

//How do I properly handle cases where I want to filter out NULL data?
//You can use filter() and provide similar syntax as you would with a SQL query.


val null_item_schema = StructType(Array(StructField("col1", StringType, true),
                               StructField("col2", IntegerType, true)))

							   
val null_dataset = sc.parallelize(Array(("test", 1 ), (null, 2))).map(x => Row.fromTuple(x))
val null_df = spark.createDataFrame(null_dataset, null_item_schema)
display(null_df.filter("col1 IS NOT NULL"))  //important

Be careful

//fromSeq---Array("hi","anu","rt")---Sequence of one type  Array[String] 
//fromTuple--Array(String, Int)------Sequence of many type Product  Array(("test", 1 ), (null, 2))

There are multiple ways of accessing Row values including Row.get* methods, Row.toSeq etc. New Row can be created using Row.apply, Row.fromSeq, Row.fromTuple or RowFactory. For example:

def transformRow(row: Row): Row =  Row.fromSeq(row.toSeq ++ Array[Any](-1, 1))

_______________________________________________________________________________________________


//How do I infer the schema using the CSV or spark-avro libraries?
//There is an inferSchema option flag. Providing a header would allow you to name the columns appropriately.

val adult_df = spark.read.
    format("com.databricks.spark.csv").
    option("header", "false").
    option("inferSchema", "true").load("hdfs://sandbox-hdp.hortonworks.com:8020/tmp/dataframe_sample.csv")
adult_df.printSchema()


__________________________________________________________________________________

 Parallelized collections are created by calling SparkContext 's parallelize method on an existing collection in your driver program 
 (a Scala Seq ). The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.
 
Creating Datasets
To convert a sequence to a Dataset, call .toDS() on the sequence.


val dataset = Seq(1, 2, 3).toDS()
dataset.show()
If you have a sequence of case classes, calling .toDS() provides a Dataset with all the necessary fields in the Dataset.

case class Person(name: String, age: Int)

val personDS = Seq(Person("Max", 33), Person("Adam", 32), Person("Muller", 62)).toDS()
personDS.show()
Create a Dataset from an RDD
To convert an RDD into a Dataset, call rdd.toDS().

val rdd = sc.parallelize(Seq((1, "Spark"), (2, "Databricks")))
val integerDS = rdd.toDS()
integerDS.show()

//DS vs DF

DS are extension of DF..Since aggregations and logical operations can be done in DS
 
scala> val rdd = sc.parallelize(Seq((1, "Spark"), (2, "Databricks")))
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[133] at parallelize at <console>:47

scala> val integerDS = rdd.toDS()
integerDS: org.apache.spark.sql.Dataset[(Int, String)] = [_1: int, _2: string]

scala> val integerDS = rdd.toDF()
integerDS: org.apache.spark.sql.DataFrame = [_1: int, _2: string]


//Create a Dataset from a DataFrame
You can call df.as[SomeCaseClass] to convert the DataFrame to a Dataset.

case class Company(name: String, foundingYear: Int, numEmployees: Int)
val inputSeq = Seq(Company("ABC", 1998, 310), Company("XYZ", 1983, 904), Company("NOP", 2005, 83))
val df = sc.parallelize(inputSeq).toDF()

df: org.apache.spark.sql.DataFrame = [name: string, foundingYear: int ... 1 more field]

val companyDS = df.as[Company]
companyDS.show()

companyDS: org.apache.spark.sql.Dataset[Company] = [name: string, foundingYear: int ... 1 more field]

You can also deal with tuples while converting a DataFrame to Dataset without using a case class.

//
val rdd = sc.parallelize(Seq((1, "Spark"), (2, "Databricks"), (3, "Notebook")))
val df = rdd.toDF("Id", "Name")

rdd.toDF.show //without any column name)
res83: org.apache.spark.sql.DataFrame = [_1: int, _2: string]

+---+----------+
| _1|        _2|
+---+----------+
|  1|     Spark|
|  2|Databricks|
|  3|  Notebook|
+---+----------+


 
val dataset = df.as[(Int, String)]
dataset.show()
dataset: org.apache.spark.sql.Dataset[(Int, String)] = [Id: int, Name: string]

__________________________________________________________________________________________________________


val wordsDataset = sc.parallelize(Seq("Spark I am your father", "May the spark be with you", "Spark I am your father")).toDS()


scala> val wordsDataset=Seq("Spark I am your father", "May the spark be with you", "Spark I am your father").toDF()
wordsDataset: org.apache.spark.sql.DataFrame = [value: string]        

//convert into DS to make it work DF won't work

//By default it it makes as values

val groupedDataset = wordsDataset.flatMap(_.toLowerCase.split(" "))
                                 .filter(_ != "")
                                 .groupBy("value")
val countsDataset = groupedDataset.count()
countsDataset.show()


//map vs flatMap

Hence, from the comparison between Spark map() vs flatMap(), it is clear that Spark map function expresses a one-to-one transformation. It transforms each element of a collection into one element of the resulting collection. While Spark flatMap function expresses a one-to-many transformation. It transforms each element to 0 or more elements.

hadoop is fast
hive is sql on hdfs
spark is superfast
spark is awesome

map
>>> wc = data.map(lambda line:line.split(" "));
>>> wc.collect()
[u'hadoop is fast', u'hive is sql on hdfs', u'spark is superfast', u'spark is awesome']

flatMap
>>> fm = data.flatMap(lambda line:line.split(" "));
>>> fm.collect()
[u'hadoop', u'is', u'fast', u'hive', u'is', u'sql', u'on', u'hdfs', u'spark', u'is', u'superfast', u'spark', u'is', u'awesome']


//word count use flatMap not map

__________________________________________________________________________________

case class Employee(name: String, age: Int, departmentId: Int, salary: Double)
case class Department(id: Int, name: String)

case class Record(name: String, age: Int, salary: Double, departmentId: Int, departmentName: String)
case class ResultSet(departmentId: Int, departmentName: String, avgSalary: Double)

val employeeDataSet1 = sc.parallelize(Seq(Employee("Max", 22, 1, 100000.0), Employee("Adam", 33, 2, 93000.0), Employee("Eve", 35, 2, 89999.0), Employee("Muller", 39, 3, 120000.0))).toDS()
val employeeDataSet2 = sc.parallelize(Seq(Employee("John", 26, 1, 990000.0), Employee("Joe", 38, 3, 115000.0))).toDS()
val departmentDataSet = sc.parallelize(Seq(Department(1, "Engineering"), Department(2, "Marketing"), Department(3, "Sales"))).toDS()

val employeeDataset = employeeDataSet1.union(employeeDataSet2)

def averageSalary(key: (Int, String), iterator: Iterator[Record]): ResultSet = {
  val (total, count) = iterator.foldLeft(0.0, 0.0) {
      case ((total, count), x) => (total + x.salary, count + 1)
  }
  ResultSet(key._1, key._2, total/count)
}

val averageSalaryDataset = employeeDataset.joinWith(departmentDataSet, $"departmentId" === $"id", "inner")
                                          .map(record => Record(record._1.name, record._1.age, record._1.salary, record._1.departmentId, record._2.name))
                                          .filter(record => record.age > 25)
                                          .groupBy($"departmentId", $"departmentName")
                                          .avg()

averageSalaryDataset.show()


____________________________________________________________________________________________________________

5.
Print two space-separated long integers denoting the respective minimum and maximum values that can be calculated by summing exactly four of the five integers. (The output can be greater than a 32 bit integer.)
Sample Input
1 2 3 4 5


Sample Output
10 14

val nums = arr.sorted ;
        val mini = nums.take(4).sum;
        val maxi = nums.drop(1).sum;

		
6.
You are in-charge of the cake for your niece's birthday and have decided the cake will have one candle for each year of her total age. When she blows out the candles, she’ll only be able to blow out the tallest ones. Your task is to find out how many candles she can successfully blow out.

For example, if your niece is turning  years old, and the cake will have  candles of height , , , , she will be able to blow out candles successfully, since the tallest candle is of height  and there are  such candles.

Complete the function birthdayCakeCandles that takes your niece's age and an integer array containing height of each candle as input, and return the number of candles she can successfully blow out.

Input Format

The first line contains a single integer, , denoting the number of candles on the cake. 
The second line contains  space-separated integers, where each integer  describes the height of candle .

Constraints

Output Format

Print the number of candles the can be blown out on a new line.

Sample Input 0

4
3 2 1 3
Sample Output 0

2


  def birthdayCakeCandles(n: Int, ar: Array[Int]): Int = {
        /*
         * Write your code here.
         */
        var count=0;
        var max_temp=ar.max;
          for(i<- 0 to ar.length-1) {
              if (ar(i) == max_temp)
              count=count+1;
          }
        count
    }

__________________________________________________________________________________

7. Sample Input

07:05:45PM
Sample Output

19:05:45


object Solution {

    /*
     * Complete the timeConversion function below.
     */
    def timeConversion(s: String): String = {
        /*
         * Write your code here.
         */
        var sub=s.substring(8,10)
        var orig=s.substring(0,8)
        var hr=s.substring(0,2).toInt
       var sep=orig.split(":")
        var con=""
        for(i<- 0 to sep.length-1) {
        if(i==0 && hr < 12 && sub=="PM")
        con += (hr+12).toString+":"
        else if(i==0 && hr==12 && sub=="AM")
        con += ("00").toString+":"
        else if(i==0 && hr==12 && sub=="AM")
        con += ("12").toString+":"
        else
        con += sep(i).toString+":" }
        
        con.substring(0,8)
    }

__________________________________________________________________________________

Note--No static variable in scala

Solution
Define non-static (instance) members in your class, and define members that you want to appear as “static” members in an object that has the same name as the class, and is in the same file as the class. This object is known as a companion object.

Using this approach lets you create what appear to be static members on a class (both fields and methods), as shown in this example:

// Pizza class
class Pizza (var crustType: String) {
    override def toString = "Crust type is " + crustType
}

// companion object
object Pizza {
    val CRUST_TYPE_THIN = "thin"
    val CRUST_TYPE_THICK = "thick"
    def getFoo = "Foo"
}
With the Pizza class and Pizza object defined in the same file (presumably named Pizza.scala), members of the Pizza object can be accessed just as static members of a Java class:

println(Pizza.CRUST_TYPE_THIN)
println(Pizza.getFoo)
You can also create a new Pizza instance and use it as usual:

var p = new Pizza(Pizza.CRUST_TYPE_THICK)
println(p)

__________________________________________________________________________________

List 

In Scala, a List inherits from Seq, but implements Product; here is the proper definition of List :

sealed abstract class List[+A] extends AbstractSeq[A] with Product with ...
[Note: the actual definition is a tad bit more complex, in order to fit in with and make use of Scala's very powerful collection framework.]


Note that

scala> val a = Seq(1,2,3)
a: Seq[Int] = List(1, 2, 3)
Is just a short hand for:

scala> val a: Seq[Int] = List(1,2,3)
a: Seq[Int] = List(1, 2, 3)
if the container type is not specified, the underlying data structure defaults to List.


List are taken as Sequence by default if cintainer not specified to acrry the details of function.




